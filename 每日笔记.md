roslaunch cartographer_ros lidar.launch
*********
```cpp
void ld_compute_cartesian(LDP ld)
{
    int i;
    for(i=0;i<ld->nrays;i++)
    {
        double x = cos(ld->theta[i]) * ld->readings[i];
        double y = sin(ld->theta[i]) * ld->readings[i];

        ld->points[i].p[0] = x, 
        ld->points[i].p[1] = y;
        ld->points[i].rho = GSL_NAN;
        ld->points[i].phi = GSL_NAN;
    }
}


void ld_compute_world_coords(LDP ld, const double *pose)
{
    double pose_x = pose[0];
    double pose_y = pose[1];
    double pose_theta = pose[2];
    double cos_theta = cos(pose_theta); 
    double sin_theta = sin(pose_theta);
    const int nrays = ld->nrays ;

    point2d * points = ld->points;
    point2d * points_w = ld->points_w;
    int i;
    for(i=0; i<nrays; i++)
    {
        if(!ld_valid_ray(ld,i))
            continue;
        double x0 = points[i].p[0],  y0 = points[i].p[1]; 
         // 省略: 判断x,y是否是nan
        // 算法第一步,获得帧yt的激光点在y(t-1)中的坐标
        points_w[i].p[0] = cos_theta * x0 -sin_theta*y0 + pose_x;
        points_w[i].p[1] = sin_theta * x0 +cos_theta*y0 + pose_y;
    }
    for(i=0;i<nrays;i++)
    {
        double x = points_w[i].p[0];
        double y = points_w[i].p[1];
        points_w[i].rho = sqrt( x*x+y*y);
        points_w[i].phi = atan2(y, x);
    }
}
```
********
```cpp
for (int j = 0; j < n ; j++)
    single_ldp->theta[j] = scan[i].min_angle + j * scan[i].angle_increment;
single_ldp->min_theta = single_ldp->theta[0];
single_ldp->max_theta = single_ldp->theta[n-1];
```
std::numeric_limits <float>::quiet_NaN ();可以得到浮点型的nan
************
```cpp
// Compute laser_sens's points in laser_ref's coordinates
//ld是params的laser_sens  pose就是params的first guess,全是0
void ld_compute_world_coords(LDP ld, const double *pose)
{
    double pose_x = pose[0];
    double pose_y = pose[1];
    double pose_theta = pose[2];
    double cos_theta = cos(pose_theta); 
    double sin_theta = sin(pose_theta);
    const int nrays = ld->nrays ;

    point2d * points = ld->points;
    point2d * points_w = ld->points_w;
    int i;
    for(i=0;i<nrays;i++)
    {
        if(!ld_valid_ray(ld,i)) continue;
        double x0 = points[i].p[0], 
               y0 = points[i].p[1];
        
        points_w[i].p[0] = cos_theta * x0 - sin_theta*y0 + pose_x;
        points_w[i].p[1] = sin_theta * x0 + cos_theta*y0 + pose_y;

        double x = points_w[i].p[0];
        double y = points_w[i].p[1];
        points_w[i].rho = sqrt( x*x+y*y);
        points_w[i].phi = atan2(y, x);
    }
}
```
*******
检查全局和局部代价地图所用的参数
**********
雷达深度传感器，基于极坐标，可以持续的读取各个离散角度方向的深度。所谓地图配准问题，就是根据机器人的传感器测量数值来计算机器人的位姿，以此最好的适配地图。
*********************
机器人移动时，机器人位置在变化，激光雷达的位置也在变化，所以每一帧的雷达数据的坐标系都不同，需要将它们统一到同一个坐标系下。雷达数据的特征提取和匹配是机器人的初定位。
*********
`PointAccumulator`定义在smmap.h

for循环显然可以用OpenMP进行并行化，github上有OpenMP优化的gmapping
********
点云转成八叉树图然后再导航

如果要在3D点云地图下面做导航，建议直接在github下搜"3D navigation"，并且多找点论文

做3D-SLAM的高效方式是OctoMap，它是基于八叉树对环境进行3D建模的方法。
使用RGBD相机导航分两大流派：

- 直接利用Kinect的3D点云做避障、路径规划：[sbpl](http://wiki.ros.org/sbpl)  [3d_navigation](http://wiki.ros.org/3d_navigation)

得到点云后，最朴素的避障就是根据你移动机器人放置的kinect位置，设置高度阈值y、横轴视界阈值x和深度阈值z，然后对裁剪后的点云有效部分在水平方向上做平均，如果大于0则障碍物偏右，向左转，小于0则障碍物偏左，向右转。
**********
播放bag文件，可以查看到topic和param，它们就存在bag文件里面。但是没有node,它是可执行文件。

**********
## 手持kinect建图

流程:  kinect——深度图——scan——laser_scan_matcher——gmapping

[rqt节点关系图](![14272971684301291.png](https://i.loli.net/2020/06/09/9aZfunEO24BTUCQ.png)

手持kinect没有里程计信息，可以用`laser_scan_matcher`做里程计估计器，它可以输出一个模拟的里程计信息，实际就是参数`fixed_frame`的值

建图时要保证kinect的水平，如果有一定程度的倾斜或翻转，建的地图就有误差。gmapping算法的时间与kinect的移动速度和环境面积有关，所以不要移动太快。

**********
`laser_scan_matcher`是一个增量式的scan配准工具. 在连续的`sensor_msgs/LaserScan`消息之间进行**scan match**, 把laser的估计位姿以`geometry_msgs/Pose2D`消息发布，产生tf变换。

可以不需要里程计信息的输入，当然也可以提供里程计以提高配准的速度和精度。

可以支持`sensor_msgs/PointCloud2`消息，但要保证没有nan值

包里的demo.bag是手持一个Hokuyo雷达的移动结果，当然没有里程计。

### 话题和tf

订阅:
- scan (sensor_msgs/LaserScan)  Scans from a laser range-finder
- cloud (sensor_msgs/PointCloud2)  Scans in point cloud form
- imu/data (sensor_msgs/Imu) Imu messages, used for theta prediction. Only used if 参数`use_imu` is set to true.
- odom (nav_msgs/Odometry) Odometry messages, used for x-, y-, and theta prediction. Only used if 参数`use_odom` is set to true.

发布:
- pose2D (geometry_msgs/Pose2D) The pose of the base frame, in some fixed (world) frame

- 需要的tf: base_link——laser, the pose of the laser in the base frame.

- 提供的tf: world——base_link, the pose of the robot base in the world frame. Only provided when publish_tf is enabled.

### 参数

- `~fixed_frame` (string, default: "world"): the fixed frame，可以改为odom
- `~base_frame` (string, default: "base_link"): the base frame of the robot

#### Motion prediction

- `~use_imu` (bool, default: true) Whether to use an imu for the theta prediction of the scan registration. Requires input on /imu/data topic.

- `~use_odom` (bool, default: true)  Whether to use wheel odometry for the x-, y-, and theta prediction of the scan registration. Requires input on odom topic.

- `~use_vel` (bool, default: false)  Whether to use constant velocity model for the x-, y-, and theta prediction of the scan registration. Requires input on vel topic.


[官网的剩余参数](http://wiki.ros.org/laser_scan_matcher)
*********
laser_scan_matcher does not function good enough, for example in a hallway. The robot position will suddenly jump to a very different(wrong) point on the map.

laser scan matcher always has the best odom in my experience, and I only want to prevent a big jump when laser_scan_matcher does not recognize it's scans anymore.